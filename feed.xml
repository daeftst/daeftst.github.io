<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://niclasdern.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://niclasdern.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-28T19:44:33+00:00</updated><id>https://niclasdern.com/feed.xml</id><title type="html">blank</title><subtitle>Website of Niclas Dern. </subtitle><entry><title type="html">Reflections on Starting Out as a Young Researcher in a Rapidly Changing World</title><link href="https://niclasdern.com/blog/2025/reflections-on-starting-out-as-a-young-researcher/" rel="alternate" type="text/html" title="Reflections on Starting Out as a Young Researcher in a Rapidly Changing World"/><published>2025-11-27T12:00:00+00:00</published><updated>2025-11-27T12:00:00+00:00</updated><id>https://niclasdern.com/blog/2025/reflections-on-starting-out-as-a-young-researcher</id><content type="html" xml:base="https://niclasdern.com/blog/2025/reflections-on-starting-out-as-a-young-researcher/"><![CDATA[<p>As I am starting out as a young researcher, I naturally have many questions about how to do great research.</p> <p>One of the most famous pieces of advice on this topic is Hamming’s talk titled ‘<a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">You and Your Research</a>’. In his talk, he among other things argues that great scientists</p> <ul> <li>have a tremendous drive and work very hard,</li> <li>work on important problems by regularly asking themselves what the most important problems are and then having the courage to pursue them,</li> <li>and are good at selling their work, collaborating with others, and working with the system rather than against it.</li> </ul> <p>I think that developing these traits and practices is very relevant to this day, if one wants to do great research. At the same time, the question that occupies my mind a lot these days goes slightly further:</p> <blockquote> <p>What additional considerations matter for doing great research in <em>our rapidly changing world</em>?</p> </blockquote> <h2 id="what-i-mean-by-great-research-and-a-rapidly-changing-world">What I mean by ‘great research’ and ‘a rapidly changing world’</h2> <p>Taking a step back, like Hamming, by ‘great research’ I mean the kind of research that years later is perceived as significant. I believe that most people reading this will have a reasonably clear idea of what it means when work is considered to be significant, though it is of course a lot harder to judge that at the time the work is done.</p> <p><strong>When I refer to our ‘rapidly changing world’, what I primarily mean is the world we will experience over the next five to ten years as the capabilities of AI models grow.</strong> Already today, models are able to reason through relatively complex problems and to work on multi-step tasks (e.g., internet research, software development) using a palette of tools. We are also seeing first signs of models being able to directly <a href="https://cdn.openai.com/pdf/4a25f921-e4e0-479a-9b38-5367b47e8fd0/early-science-acceleration-experiments-with-gpt-5.pdf">accelerate scientific work</a> and signs that the US government is interested in further increasing these abilities through <a href="https://www.nature.com/articles/d41586-025-03890-z">coordinated national efforts</a>. It does not currently seem to me as if the growth in model capabilities will be slowing down anytime soon, and I personally believe that models are likely to reach a level where they can automate the vast majority of the currently ongoing scientific work within the next 5-15 years.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p> <p>Even if growth is slower or if we do not eventually get to the level of capabilities where we can automate all scientific workflows, I very strongly expect that we will in the next few years develop tools that automate <em>many parts</em> of the scientific process.</p> <h2 id="do-rapid-developments-change-what-makes-research-significant">Do rapid developments change <em>what makes</em> research significant?</h2> <blockquote> <p>Imagine going out for dinner with your (ex-)scientist friends in 10 years. One of your friends asks what you think was the most important research that was done in your field in the last decade. You pause for a bit and start thinking.</p> </blockquote> <p>Generally, I do not think that what makes research significant will change drastically from what has historically mattered. Making previously impossible-to-answer questions tractable, connecting seemingly disparate fields, resolving fundamental puzzles in relevant fields, or developing new methods that enable a lot of further research will all remain important.</p> <p>At the same time, there are at least two (rough) kinds of research that I think will become <em>more important</em> over the next years.</p> <p>Firstly, I think research that enables the scientific community and society to better ensure that rapidly developing new technologies do not lead to major harm will become increasingly important.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> This similarly applies for work that helps to make sure that the increasing number of scientific results being produced are <a href="https://en.wikipedia.org/wiki/Replication_crisis">reproducible</a> and robust.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p> <p>Furthermore, in the spirit of <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">the bitter lesson</a>, I believe that research that develops methods which are able to efficiently leverage resources that will become abundant over the next years – such as <em>automated reasoning, compute, and (synthetic) data</em> – will become more important. On the flip side, this also means there is increased danger of work becoming obsolete at or even before the end of its development. This can happen when massively more resources, which the work does not efficiently leverage, have become available by the time the it is completed.</p> <h2 id="how-will-the-approaches-to-do-great-research-change">How will the approaches <em>to do</em> great research change?</h2> <blockquote> <p>Imagine you are sitting in your office several years from now a few days before Christmas. You feel like the past year was by far the most productive in your scientific career and that you were able to put out research that you think will have a lasting impact. You start to reflect on what the approaches were that enabled you to be as productive in the last year.</p> </blockquote> <p>While I expect what makes research significant not to change drastically over the next decade, I think the approaches to do great research will change a great deal.</p> <p>On a high level, my view is that <strong>approaches to doing great research should focus increasingly on <em>effectively</em> leveraging the aforementioned resources (automated reasoning, compute, and data) that will become massively more abundant over the next years</strong>. Here, I emphasize the <em>effectively</em> since I believe that it will often not be very clear over the next years how to get the most benefits from these resources.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/leveraging_the_tools-480.webp 480w,/assets/img/leveraging_the_tools-800.webp 800w,/assets/img/leveraging_the_tools-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/leveraging_the_tools.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Approaches to doing great research should increasingly focus on leveraging the resources (such as automated reasoning) that will become massively more abundant over the next years. But it is important not to overdo it and to focus on using them effectively. (Image generated using Nano Banana Pro.)</figcaption> </figure> <p>Practically, I think leveraging the new resources available will amount to using new or existing products (such as LLM-Chatbots, AI Coding Agents, Literature Research Tools or platforms for compute orchestration) or building your own software/hardware to support parts of your research process.</p> <p>Figuring out how to leverage these tools <em>effectively</em> though will likely be lot harder.</p> <p><strong>The most important step in finding out what works will likely be to do a significant amount of experimentation.</strong> Given how fast things are moving, I would argue that one should probably constantly be experimenting a bit and should be open to frequently changing one’s research processes. Having a community of good researchers going through the same process, who can support each other and share insights on using new tools, will definitely be very helpful. Furthermore, drawing inspiration from friends who are not involved in research directly, but are also in the process of figuring out how to make the most of the new capabilities, can also be very valuable.</p> <p><strong>When reflecting specifically on your usage of new <em>AI models</em>, I think one particularly relevant aspect to consider is which complementary strengths you currently have or want to develop.</strong> I believe you should evaluate these both in comparison to current and future models, and relative to other researchers who are also using them. In comparison to models, some general complementary strengths good researchers have and will likely retain in the coming years are, in my view, better judgement in (a) determining which questions are worth pursuing and (b) selecting approaches to solve sub-problems, as well as (c) learning in real-time and applying that new understanding to current and future problems, and (d) high-quality writing.</p> <p>Since it is pretty hard to think about how to use the new tools effectively ‘to do great research’, <strong>I believe a reasonable proxy to often optimize for is getting more ambitious research results faster</strong> (where results can mean empirical findings, theoretical insights, or simply a deeper understanding). More ambition hopefully leads to working on more important questions, while getting faster results leads to you being able to profit more from the <a href="https://en.wikipedia.org/wiki/Exponential_function">compound interest</a> of your work.</p> <p>As a last point of warning, <strong>two failure modes to avoid when using these new tools are producing more research rather than better research, and failing to develop a deep enough understanding of the details of your work.</strong> The latter point, in my opinion, is especially easy to fall into and bears the risk that a lack of deep understanding prevents you from developing good judgement on problems and approaches to them that might ultimately form your comparative advantage over AI models.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> It might also leave you unable to verify in detail whether results are correct in the future.</p> <h2 id="concluding-thoughts">Concluding thoughts</h2> <p>Reflecting on the fact that I am starting out as a researcher in a rapidly changing world often feels weird (and sometimes even scares me a bit). To be honest, I have no idea what exactly my job will look like at the end of my PhD. At the same time, I also feel extremely grateful for being able to take part in shaping how science will be done and for living through these very exciting times, in which we might be able to understand our world more deeply and develop more powerful tools to shape it than ever before!</p> <hr/> <p><em>Thanks to Jennifer Zhang and Kilian Pschierer for feedback on drafts of this post. Thanks also to Jascha Sohl-Dickstein, whose <a href="https://x.com/jaschasd/status/1972360405885637021">talk on this topic</a> inspired some of the thoughts in this text.</em></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I believe that we will be able to automate work that can be done solely on a computer faster than research that requires physical interaction (e.g., running experiments in a wet lab). Nevertheless, I think it is likely that we will be able to automate both kinds of work within the next 5-15 years. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>Note that if we reach a level where AIs can automate most scientific work, this would not necessarily mean that all scientific work will only be done by AI models. For example, humans might still have an advantage in some parts of the scientific workflow, or from a societal perspective, we might still want humans to have the final say in certain decisions and/or to verify results. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>By new technologies potentially leading to major harm, I mean harm that might come from widespread usage of a technology without sufficient deliberation, from malicious use, or from unintended malfunctions. Areas in which advances might lead to such harm include, for example, biotechnology, geoengineering, or advances in AI capabilities directly (potentially leading to <a href="https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/">catastrophic risks</a> or <a href="https://gradual-disempowerment.ai/">gradual disempowerment</a>). <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Note that the generation of more research results might offer a natural opportunity for better reproducibility checks, since new results building upon or comparing to old results will often require reproducing previous findings in slightly perturbed settings. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p>The question of where to draw the line between failing to develop the deep understanding needed to improve your judgement, and not using new capabilities enough to get feedback on that judgement, is a hard but important one. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="research"/><category term="ai"/><category term="reflection"/><summary type="html"><![CDATA[Thoughts on how to do great research in a world where the capabilities of AI models grow quickly.]]></summary></entry><entry><title type="html">Toward More Deliberate AI Use</title><link href="https://niclasdern.com/blog/2025/toward-more-deliberate-ai-use/" rel="alternate" type="text/html" title="Toward More Deliberate AI Use"/><published>2025-08-24T12:00:00+00:00</published><updated>2025-08-24T12:00:00+00:00</updated><id>https://niclasdern.com/blog/2025/toward-more-deliberate-ai-use</id><content type="html" xml:base="https://niclasdern.com/blog/2025/toward-more-deliberate-ai-use/"><![CDATA[<p>I have been using AI extensively in recent months. I use it to help me write text, develop code, create designs for presentations and posters, and more generally, to answer all kinds of questions about the world around me. I rely on it especially often when I am uncertain about how to approach a task or when I lack energy.</p> <p>One example of how I interact with AI models is that I copy small paragraphs of text to a chat interface and ask the AI model to “Shorten this slightly,” “Please add…” or “Rewrite this based on the following feedback…”. For code, I often don’t even write my initial implementation – I usually say something like “Here is my code so far: … I would now like to implement…” and then sometimes correct the model by saying “No, please implement this in the following way…”.</p> <p>This way of interacting with AI has several important issues. Firstly, producing content while using extensive AI assistance is often much less enjoyable on a deeper, long-term level. I feel far less connected to the content I produce by copy-pasting AI-generated text and code. Secondly, some of the content I produce this way is structurally worse. Even though AI models are becoming generally smarter and can hold more information in their context, they still often lack the ability to make good decisions in writing, software development, and many larger projects. Since I am often too lazy (and too focused on details) to revert a design decision once it’s made, the products of ‘my’ work end up being lower quality. Third, I tend to learn much less from creating new things this way – for example, about the specific software framework I use. This sometimes limits my ability to quickly understand and fix issues with current code or text, to iterate fast over multiple possible next steps, and more generally constrains the growth of my skills.</p> <p>I don’t want to suggest that one should completely avoid using AI models. The ability to get relatively high-quality answers on problems that require substantial context also enables me to learn many new things. I can probably complete more and significantly larger projects with AI’s help, and it’s often very useful to map out multiple approaches to something in detail before deciding on the next step. However, I think the way I interact with AI should become more deliberate.</p> <p>Going forward, I would like to make using AI a more active decision. I want to think consciously about whether I really want to solve a particular problem using AI, and if so, how I would like to use AI in that context. As a general guideline, I would like to write the first full draft of every text that matters to me myself. When writing code, I would like to write most short snippets of code myself (this might sometimes include rewriting code that an AI model provided in a chat interface, in case I did not know the syntax of the library/framework I am using). Furthermore, every once in a while, I would like to pick some code or text where I am not confident I could have written it as quickly myself, and delete and re-implement or rewrite it.</p>]]></content><author><name></name></author><category term="blog"/><category term="ai"/><category term="productivity"/><category term="reflection"/><summary type="html"><![CDATA[Why and how I'm changing my AI use to be more intentional]]></summary></entry></feed>